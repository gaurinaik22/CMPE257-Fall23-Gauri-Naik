# -*- coding: utf-8 -*-
"""HW2Task3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14Rn2D8JvjqPdNRdaZkCcL_PHmWnN9-DB
"""

import pandas as pd
import numpy as np

from google.colab import drive
drive.mount('/content/drive')

df_train  = pd.read_csv('/content/drive/MyDrive/ML/ZipDigits.train' , header=None, sep=' ')

df_train.head()

df_train.shape

df_test = pd.read_csv('/content/drive/MyDrive/ML/ZipDigits.test', header=None, sep=' ')

df_test.head()

df_test.shape

"""Removing NAN from the dataset column 257"""

df_train = df_train.drop(df_train.columns[257], axis=1)
df_test = df_test.drop(df_test.columns[257], axis=1)

df_train

print(df_train.shape)
print(df_test.shape)

df_train.describe()

df_train.info()

df_train.dtypes

df_Filtered_train = df_train.copy()

df_Filtered_train = df_Filtered_train[(df_Filtered_train[0] == 1.0) | (df_Filtered_train[0] == 5.0)]

df_Filtered_train

df_Filtered_train[[0]].value_counts()

df_Filtered_train.loc[df_Filtered_train[0] == 1.0, 0] = 1
df_Filtered_train.loc[df_Filtered_train[0] == 5.0, 0] = -1

df_Filtered_train[[0]].value_counts()

df_Filtered_train.head()

df_Filtered_train[[0]].value_counts()

df_Filtered_test = df_test.copy()
df_Filtered_test.head()

df_Filtered_test = df_Filtered_test[(df_Filtered_test[0] == 1.0) | (df_Filtered_test[0] == 5.0)]
df_Filtered_test.head()

# df_Filtered_test.loc[df_Filtered_test[0] == 1.0, 0] = 1
# df_Filtered_test.loc[df_Filtered_test[0] == 5.0, 0] = -1
# df_Filtered_test

# Split the DataFrame into X and y using array slicing for Train dataset
X_train = df_Filtered_train.iloc[:, 1:]
y_train = df_Filtered_train.iloc[:, 0]

X_train.shape

y_train.shape

# Split the DataFrame into X and y using array slicing for Test dataset
X_test = df_Filtered_test.iloc[:, 1:]
y_test = df_Filtered_test.iloc[:, 0]

X_test.shape

y_test.shape

# reshape the data into 16x16 images
X_train = X_train.values.reshape(-1, 16, 16)
X_test = X_test.values.reshape(-1, 16, 16)
y_train = y_train.values.reshape(-1, 1)
y_test = y_test.values.reshape(-1, 1)

print(X_train.shape)
print(y_train.shape)

print(X_test.shape)
print(y_test.shape)

import matplotlib.pyplot as plt

# Create a subplot for each image
fig, axes = plt.subplots(2, 5, figsize=(10, 5))
for i, ax in enumerate(axes.flat):
    ax.imshow(X_train[i], cmap='gray')
    ax.set_xticks([])
    ax.set_yticks([])

# Create a subplot for each image
fig, axes = plt.subplots(2, 5, figsize=(10, 5))
for i, ax in enumerate(axes.flat):
    ax.imshow(X_test[i], cmap='gray')
    ax.set_xticks([])
    ax.set_yticks([])

def calculateIntensity(gray_image):
    return np.mean(gray_image)

pip install opencv-python

import cv2

def calculateSymmetry(gray_image):
    flipped_image = cv2.flip(gray_image, 1)
    difference = cv2.absdiff(gray_image, flipped_image)
    return np.mean(difference)

trainIntensities = []
trainSymmetries = []

for i in X_train:
    trainIntensities.append(calculateIntensity(i))
    trainSymmetries.append(calculateSymmetry(i))

testIntensities = []
testSymmetries = []

for i in X_test:
    testIntensities.append(calculateIntensity(i))
    testSymmetries.append(calculateSymmetry(i))

len(trainIntensities)

len(trainSymmetries)

len(testIntensities)

len(testSymmetries)

trainIntensities[:10], trainSymmetries[:10]

df_Filtered_train['intensity'] = trainIntensities
df_Filtered_train['symmetry'] = trainSymmetries

df_Filtered_test['intensity'] = testIntensities
df_Filtered_test['symmetry'] = testSymmetries

""" # plot the symmetry vs target, red for 1 and blue for 5"""

plt.figure(figsize=(10, 5))
plt.scatter(df_Filtered_train['intensity'], df_Filtered_train[0], c=df_Filtered_train[0], cmap='bwr')
plt.xlabel('Intensity')
plt.ylabel('Target')
plt.title('Intensity vs Target')
plt.show()

""" # plot the intensity vs symmetry, red for 1 and blue for 5"""

plt.figure(figsize=(10, 5))
plt.scatter(df_Filtered_train['symmetry'], df_Filtered_train[0], c=df_Filtered_train[0], cmap='bwr')
plt.xlabel('Symmetry')
plt.ylabel('Target')
plt.title('Symmetry vs Target')
plt.show()

import numpy as np

plt.figure(figsize=(10, 5))
colormap = np.array(['r', 'g'])
plt.scatter(df_Filtered_train['intensity'], df_Filtered_train['symmetry'], c=df_Filtered_train[0], cmap='bwr')
plt.xlabel('Intensity')
plt.ylabel('Symmetry')
plt.title('Intensity vs Symmetry')
# add the legend to the plot, 1 for red and 5 for blue
plt.legend(handles=[plt.scatter([], [], c='blue', label='1'), plt.scatter([],[], c='red', label='5')], title='Target')
plt.show()

"""#We see that intensity is proportional to symmetry

PLA Algorithm on Preprocessed digits dataset limiting the number of updates to 1000 iterations
"""

train_1000iter = pd.DataFrame({'x1': df_Filtered_train['intensity'], 'x2': df_Filtered_train['symmetry'], 'y':df_Filtered_train[0]})

test_1000iter = pd.DataFrame({'x1': df_Filtered_test['intensity'], 'x2': df_Filtered_test['symmetry'], 'y':df_Filtered_test[0]})

train_1000iter.head()

test_1000iter.head()

train_1000iter['x0'] = 1
test_1000iter['x0'] = 1
train_1000iter = train_1000iter[['x0', 'x1', 'x2', 'y']]
test_1000iter = test_1000iter[['x0', 'x1', 'x2', 'y']]

train_1000iter.head()

test_1000iter.head()



# Accuracy on dataset
def get_accuracy(dataframe, weights):
  correct_cnt = 0
  for idx in range(dataframe.shape[0]):
    x, y = dataframe.iloc[idx, :dataframe.shape[1] - 1].values, dataframe.iloc[idx, dataframe.shape[1] - 1]
    st = np.dot(weights.T, x)
    if y * st >= 0:
      correct_cnt += 1
  print(f"Number of correct predictions: {correct_cnt}")
  return correct_cnt / dataframe.shape[0] * 100



def perceptron(points, is_pocket=False, max_iterations=1000, lr=0.001):
  w = np.zeros(points.shape[1] - 1)
  best_w = np.zeros(points.shape[1] - 1)
  best_correct_count = 0
  xs, ys = points[:, :points.shape[1] - 1], points[:, points.shape[1] - 1]
  num_points = points.shape[0]
  iterations = 0

  for it in range(max_iterations):
    correct_count = 0
    indexes = np.arange(num_points)
    misclassified = False
    iterations += 1
    for idx in indexes:
      x, y = xs[idx], ys[idx]
      st = np.dot(w.T, x)
      prod = st * y
      if prod <= 0:
        w = w + lr * y * x
        misclassified = True
      else:
        correct_count += 1
    if is_pocket and correct_count > best_correct_count:
      best_w = w.copy()
      best_correct_count = correct_count
    if not misclassified:
      break

  if is_pocket:
    print(w, correct_count, best_correct_count)
    w = best_w
    correct_count = best_correct_count

  w = w / np.linalg.norm(w)
  return w, iterations, correct_count

def plot_data(data, actual=None, hypothesis=None):
  f, ax = plt.subplots(1, 1)

  x1 = np.arange(-1, 0.25, 0.01)
  if actual:
    ax.plot(x1, -(actual[0] + actual[1] * x1), label='f(x)')

  ax.scatter(data['x1'], data['x2'], c=data['y'], s=1, cmap='bwr')
  if hypothesis is not None:
    ax.plot(x1, -(hypothesis[0] + hypothesis[1] * x1) / hypothesis[2],c='green', label='g(x)')
  ax.set_xlabel("x")
  ax.set_ylabel("y")
  ax.set_title('Data set size = %s' % data.shape[0])
  ax.set_xlim([-1, 0.25])
  ax.set_ylim([-0.25, 1])
  plt.legend(loc='best')
  plt.show()

"""1.0.1 (a) Run your PLA algorithm (from Class Activity 1) on the preprocessed digits
dataset from Task 4 of HW 1 limiting the number o updates to 1000 iterations
and plot the resulting final hypothesis along with the data points
"""

w, iterations, correct_count = perceptron(train_1000iter.values, is_pocket=False,max_iterations=1000)

print(f"Number of iterations: {iterations}\nNumber of correct predictions:{correct_count}")
print(f"Number of iterations: {iterations}\nNumber of correct predictions:{correct_count}")

plot_data(train_1000iter, None, w)

# Accuracy on test set
get_accuracy(test_1000iter, w)

"""1.0.2 (b) Repeat the experiment with 1000 updates using the pocket algorithm and
plot the resulting final hypothesis along with the data points.

"""

w, iterations, correct_count = perceptron(train_1000iter.values, is_pocket=True,max_iterations=1000)

print(f"Number of iterations: {iterations}\nNumber of correct predictions:{correct_count}")

plot_data(train_1000iter, None, w)

get_accuracy(test_1000iter, w)

""" LP 2 - Use a third order polynomial feature transformation and run the
pocket algorithm for 1000 updates on the digits dataset. Report the test
error and plot the resulting final hypothesis with the data points.

"""

train_1000iter.head()

from sklearn.preprocessing import PolynomialFeatures

