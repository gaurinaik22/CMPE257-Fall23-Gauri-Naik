# -*- coding: utf-8 -*-
"""HW2-Task 2 Problem 3.1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZVieGS2b_r9mp4FwXldRnF2IQfd9E3ab
"""

#### CREDITS: Wencen Wu ####
import numpy as np

#parameters
rad = 10
thk = 5
sep = 5

#n data points,(x1,y1) are the coordinates of the top semi-circle
def generatedata(rad,thk,sep,n,x1 = 0,y1 = 0):
    # center of the top semi-circle
    X1 = x1
    Y1 = y1

    # center of the bottom semi-circle
    X2 = X1 + rad + thk / 2
    Y2 = Y1 - sep

    # data points in the top semi-circle
    top = []
    # data points in the bottom semi-circle
    bottom = []

    # parameters
    r1 = rad + thk
    r2 = rad

    cnt = 1
    while(cnt <= n):
        #uniformed generated points
        x = np.random.uniform(-r1,r1)
        y = np.random.uniform(-r1,r1)

        d = x**2 + y**2
        if(d >= r2**2 and d <= r1**2):
            if (y > 0):
                top.append([X1 + x,Y1 + y])
                cnt += 1
            else:
                bottom.append([X2 + x,Y2 + y])
                cnt += 1
        else:
            continue

    return top,bottom

import matplotlib.pyplot as plt
top,bottom = generatedata(rad,thk,sep,1000)

X1 = [i[0] for i in top]
Y1 = [i[1] for i in top]

X2 = [i[0] for i in bottom]
Y2 = [i[1] for i in bottom]


plt.scatter(X1,Y1,s = 1)
plt.scatter(X2,Y2,s = 1)
plt.show()

#put the data into a dataframe
import pandas as pd
df_top = pd.DataFrame(top)
df_bottom = pd.DataFrame(bottom)

df_top['label'] = 1
df_bottom['label'] = -1

df = pd.concat([df_top,df_bottom],axis = 0)
df.columns = ["x1","x2",'y']

df.head()

X=df[["x1","x2"]]
x0 = [1]*len(X)
X.insert(0,"x0",x0)
y=df['y']

X

#X = np.c_[np.ones((X.shape[0], 1)), X]

X.iloc[1]

y

#PLA
def PLA(X, y, maxIteration=1000):
    # Initialize weights to zeros
    w = np.zeros(X.shape[1])
    misClassified = True
    iterationCount = 0
    while misClassified and iterationCount < maxIteration:
        misClassified = False
        for i in range(X.shape[0]):
            x_curr = np.array(X.iloc[i])
            if np.dot(x_curr, w) * y[i] <= 0:
                w += y[i] * x_curr
                misClassified = True

        # Check if all points are correctly classified
        allCorrect = True
        for i in range(X.shape[0]):
            x_curr = np.array(X.iloc[i])
            if np.dot(x_curr, w) * y[i] <= 0:
                allCorrect = False
                break

        if allCorrect:
            break

        iterationCount += 1

    print(w)
    return w

y = list(y)
wPLA = PLA(X, y)

wPLA

X

X = np.c_[np.ones((X.shape[0], 1)), X]

plt.figure(figsize=(10, 6))
plt.scatter(X1,Y1,s = 1)
plt.scatter(X2,Y2,s = 1)

#x_vals = np.linspace(np.min(X[:, 1]), np.max(X[:, 1]), 100)
x_vals = np.linspace(-30, 30, 100)
plt.plot(x_vals, (-wPLA[0] - wPLA[1] * x_vals) / wPLA[2], '--', label='PLA')


plt.xlabel('X1')
plt.ylabel('X2')
plt.title('Dataset and Final Hypotheses')
plt.legend()
plt.grid(True)
plt.show()



X

y

X1 = [[i[1], i[2]] for i in X]
Y1  = [i[0] for i in X]

df=X[['x1','x2']]

df

X1=df.values.tolist()

X1

y=list(y)
y

XtX = np.matmul(np.transpose(X1),X1)

XtX

np.linalg.inv(XtX)

B_c = np.dot(np.transpose(X1),y)

B_c

w = np.dot(np.linalg.inv(XtX),B_c)
w

f_w0, f_w1 = 1, 1
N = 50

def abline(slope, intercept):
    """Plot a line from slope and intercept"""
    axes = plt.gca()
    x_vals = np.array(axes.get_xlim())
    y_vals = intercept + slope * x_vals
    plt.plot(x_vals, y_vals, '--', label = "x_2="+str(slope)+"x_1+"+str(intercept) )

X2 = [i[0] for i in X1]
Y2  = [i[1] for i in X1]
markers = [] #'o' or '+'
colors = [] # 'b' or 'r'
cnt = 0
plt.scatter(X2,Y2, s = 10)

abline(f_w1,f_w0)
plt.show()

rad, thk = 10, 5
x1 = np.arange(-(rad+thk), (rad+thk)+rad + thk/2)
linear = ax.plot(x1, -(w[0]+w[1]*x1), c = 'r', label='Linear Regression')
ax.set_ylabel(r"$x_2$", fontsize=11)
ax.set_xlabel(r"$x_1$", fontsize=11)
ax.set_title('Data set size = %s'%N, fontsize=9)
ax.axis('tight')
legend_x = 2.0
legend_y = 0.5
ax.legend(['PLA', 'Linear Regression',
           '+1 labels', '-1 labels', ],
          loc='center right', bbox_to_anchor=(legend_x, legend_y))
#ax.set_ylim(bottom=lb, top=ub)
plt.show()

X1 = [i[1] for i in data]
Y  = [i[2] for i in data]
markers = [] #'o' or '+'
colors = [] # 'b' or 'r'
cnt = 0
plt.scatter(X1,Y, s = 10)

abline(f_w1,f_w0)
plt.show()

X = [[i[0], i[1]] for i in data]
Y  = [i[2] for i in data]

XtX = np.dot(np.transpose(X),X)
XtX_inv = np.linalg.inv(XtX)
XtX_inv

B_c = np.dot(np.transpose(X),Y)

B_c

w = np.dot(XtX_inv,B_c)
w

def mean_normalization(X):
    mean_vals = np.mean(X, axis=0)
    std_dev = np.std(X, axis=0)
    X_normalized = (X - mean_vals) / std_dev
    return X_normalized

X_scaled= mean_normalization(X)

def linear_regression_classification(X, Y, learning_rate, num_iterations):
    m, n = X.shape
    w = np.zeros(n)  # Initialize weights to zeros

    for _ in range(num_iterations):
        predictions = np.dot(X, w)
        errors = predictions - Y
        gradient = 2 * np.dot(X.T, errors) / m
        w -= learning_rate * gradient

    return w

# Set hyperparameters
learning_rate = 0.001
num_iterations = 100

# Train the model
final_weights = linear_regression_classification(X_scaled, y, learning_rate, num_iterations)

final_weights



import numpy as np


class LinearRegression:
    def __init__(self, lr: int = 0.01, n_iters: int = 1000) -> None:
        self.lr = lr
        self.n_iters = n_iters
        self.weights = None
        self.bias = None

    def fit(self, X, y):
        num_samples, num_features = X.shape     # X shape [N, f]
        self.weights = np.random.rand(num_features)  # W shape [f, 1]
        self.bias = 0

        for i in range(self.n_iters):

            # y_pred shape should be N, 1
            y_pred = np.dot(X, self.weights) + self.bias

            # X -> [N,f]
            # y_pred -> [N]
            # dw -> [f]
            dw = (1 / num_samples) * np.dot(X.T, y_pred - y)
            db = (1 / num_samples) * np.sum(y_pred - y)

            self.weights = self.weights - self.lr * dw
            self.bias = self.bias - self.lr * db

        return self

    def predict(self, X):
        return np.dot(X, self.weights) + self.bias